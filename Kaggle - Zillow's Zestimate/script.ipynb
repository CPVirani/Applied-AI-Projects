{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Parameters\nXGB_WEIGHT = 0.6415\nBASELINE_WEIGHT = 0.0050\nOLS_WEIGHT = 0.0856\n\nXGB1_WEIGHT = 0.8083  # Weight of first in combination of two XGB models\n\nBASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg\n\n\n\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc\nfrom sklearn.linear_model import LinearRegression\nimport random\nimport datetime as dt\n\n\n##### READ IN RAW DATA\n\nprint( \"\\nReading data from disk ...\")\nprop = pd.read_csv('../input/properties_2016.csv')\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")\n\n\n\n\n################\n################\n##  LightGBM  ##\n################\n################\n\n# This section is (I think) originally derived from SIDHARTH's script:\n#   https://www.kaggle.com/sidharthkumar/trying-lightgbm\n# which was forked and tuned by Yuqing Xue:\n#   https://www.kaggle.com/yuqingxue/lightgbm-85-97\n# and updated by me (Andy Harless):\n#   https://www.kaggle.com/aharless/lightgbm-with-outliers-remaining\n# and a lot of additional changes have happened since then,\n#   the most recent of which are documented in my comments above\n \n\n##### PROCESS DATA FOR LIGHTGBM\n\nprint( \"\\nProcessing data for LightGBM ...\" )\nfor c, dtype in zip(prop.columns, prop.dtypes):\t\n    if dtype == np.float64:\t\t\n        prop[c] = prop[c].astype(np.float32)\n\ndf_train = train.merge(prop, how='left', on='parcelid')\ndf_train.fillna(df_train.median(),inplace = True)\n\n##add month feature\ndf_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\ndf_train[\"Month\"] = df_train[\"transactiondate\"].dt.month\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n#x_train['Ratio_1'] = x_train['taxvaluedollarcnt']/x_train['taxamount']\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)\n\n\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\ndel df_train; gc.collect()\n\nx_train = x_train.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)\n\n\n\n##### RUN LIGHTGBM\n\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          # or 'mae'\nparams['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\nparams['bagging_fraction'] = 0.85 # sub_row\nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        # num_leaf\nparams['min_data'] = 500         # min_data_in_leaf\nparams['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n\nnp.random.seed(0)\nrandom.seed(0)\n\nprint(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)\n\ndel d_train; gc.collect()\ndel x_train; gc.collect()\n\nprint(\"\\nPrepare for LightGBM prediction ...\")\nprint(\"   Read sample file ...\")\nsample = pd.read_csv('../input/sample_submission.csv')\nprint(\"   ...\")\nsample['parcelid'] = sample['ParcelId']\nprint(\"   Merge with property data ...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\n\n####add month feature assuming 2016-10-01\ndf_test[\"transactiondate\"] = '2016-07-01'\ndf_test[\"transactiondate\"] = pd.to_datetime(df_test[\"transactiondate\"])\ndf_test[\"Month\"] = df_test[\"transactiondate\"].dt.month #should use the most common training date 2016-10-01\ndf_test = df_test.drop(['transactiondate'], axis=1)\n\nprint(\"   ...\")\ndel sample, prop; gc.collect()\nprint(\"   ...\")\n#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\nx_test = df_test[train_columns]\nprint(\"   ...\")\ndel df_test; gc.collect()\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"\\nStart LightGBM prediction ...\")\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )\n\n\n\n\n################\n################\n##  XGBoost   ##\n################\n################\n\n# This section is (I think) originally derived from Infinite Wing's script:\n#   https://www.kaggle.com/infinitewing/xgboost-without-outliers-lb-0-06463\n# inspired by this thread:\n#   https://www.kaggle.com/c/zillow-prize-1/discussion/33710\n# but the code has gone through a lot of changes since then\n\n\n##### RE-READ PROPERTIES FILE\n##### (I tried keeping a copy, but the program crashed.)\n\nprint( \"\\nRe-reading properties file ...\")\nproperties = pd.read_csv('../input/properties_2016.csv')\n\n\n\n##### PROCESS DATA FOR XGBOOST\n\nprint( \"\\nProcessing data for XGBoost ...\")\nfor c in properties.columns:\n    properties[c]=properties[c].fillna(-1)\n    if properties[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties[c].values))\n        properties[c] = lbl.transform(list(properties[c].values))\n\ntrain_df = train.merge(properties, how='left', on='parcelid')\n\ntrain_df[\"transactiondate\"] = pd.to_datetime(train_df[\"transactiondate\"])\ntrain_df[\"Month\"] = train_df[\"transactiondate\"].dt.month\n\nx_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\nx_test = properties.drop(['parcelid'], axis=1)\n\nx_test[\"transactiondate\"] = '2016-07-01'\nx_test[\"transactiondate\"] = pd.to_datetime(x_test[\"transactiondate\"])\nx_test[\"Month\"] = x_test[\"transactiondate\"].dt.month #should use the most common training date 2016-10-01\nx_test = x_test.drop(['transactiondate'], axis=1)\n\n# shape        \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n\n# drop out ouliers\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\nx_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\n\nprint('After removing outliers:')     \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n\n\n\n\n##### RUN XGBOOST\n\nprint(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.037,\n    'max_depth': 5,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\nnum_boost_rounds = 250\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\n# train model\nprint( \"\\nTraining XGBoost ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost ...\")\nxgb_pred1 = model.predict(dtest)\n\nprint( \"\\nFirst XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred1).head() )\n\n\n\n##### RUN XGBOOST AGAIN\n\nprint(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.033,\n    'max_depth': 6,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'base_score': y_mean,\n    'silent': 1\n}\n\nnum_boost_rounds = 150\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\nprint( \"\\nTraining XGBoost again ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost again ...\")\nxgb_pred2 = model.predict(dtest)\n\nprint( \"\\nSecond XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred2).head() )\n\n\n\n##### COMBINE XGBOOST RESULTS\nxgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n#xgb_pred = xgb_pred1\n\nprint( \"\\nCombined XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred).head() )\n\ndel train_df\ndel x_train\ndel x_test\ndel properties\ndel dtest\ndel dtrain\ndel xgb_pred1\ndel xgb_pred2 \ngc.collect()\n\n\n\n################\n################\n##    OLS     ##\n################\n################\n\n# This section is derived from the1owl's notebook:\n#    https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\n# which I (Andy Harless) updated and made into a script:\n#    https://www.kaggle.com/aharless/updated-script-version-of-the1owl-s-basic-ols\n\nnp.random.seed(17)\nrandom.seed(17)\n\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\nproperties = pd.read_csv(\"../input/properties_2016.csv\")\nsubmission = pd.read_csv(\"../input/sample_submission.csv\")\nprint(len(train),len(properties),len(submission))\n\ndef get_features(df):\n    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n    df['transactiondate'] = df['transactiondate'].dt.quarter\n    df = df.fillna(-1.0)\n    return df\n\ndef MAE(y, ypred):\n    #logerror=log(Zestimate)âˆ’log(SalePrice)\n    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)\n\ntrain = pd.merge(train, properties, how='left', on='parcelid')\ny = train['logerror'].values\ntest = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\nproperties = [] #memory\n\nexc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\ncol = [c for c in train.columns if c not in exc]\n\ntrain = get_features(train[col])\ntest['transactiondate'] = '2016-01-01' #should use the most common training date\ntest = get_features(test[col])\n\nreg = LinearRegression(n_jobs=-1)\nreg.fit(train, y); print('fit...')\nprint(MAE(y, reg.predict(train)))\ntrain = [];  y = [] #memory\n\ntest_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\ntest_columns = ['201610','201611','201612','201710','201711','201712']\n\n\n\n\n########################\n########################\n##  Combine and Save  ##\n########################\n########################\n\n\n##### COMBINE PREDICTIONS\n\nprint( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\nlgb_weight = (1 - XGB_WEIGHT - BASELINE_WEIGHT) / (1 - OLS_WEIGHT)\nxgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\nbaseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\npred0 = xgb_weight0*xgb_pred + baseline_weight0*BASELINE_PRED + lgb_weight*p_test\n\nprint( \"\\nCombined XGB/LGB/baseline predictions:\" )\nprint( pd.DataFrame(pred0).head() )\n\nprint( \"\\nPredicting with OLS and combining with XGB/LGB/baseline predicitons: ...\" )\nfor i in range(len(test_dates)):\n    test['transactiondate'] = test_dates[i]\n    pred = OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0\n    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n    print('predict...', i)\n\nprint( \"\\nCombined XGB/LGB/baseline/OLS predictions:\" )\nprint( submission.head() )\n\n\n\n##### WRITE THE RESULTS\n\nfrom datetime import datetime\n\nprint( \"\\nWriting results to disk ...\" )\nsubmission.to_csv('TEm07sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"%matplotlib inline\n### Seaborn style\nsns.set_style(\"whitegrid\")\n## Dictionary of feature dtypes\nints = ['parcelid']\n\nfloats = ['basementsqft', 'bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', 'finishedfloor1squarefeet', \n          'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13',\n          'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6', 'fireplacecnt',\n          'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'latitude', 'longitude',\n          'lotsizesquarefeet', 'poolcnt', 'poolsizesum', 'roomcnt', 'threequarterbathnbr', 'unitcnt',\n          'yardbuildingsqft17', 'yardbuildingsqft26', 'yearbuilt', 'numberofstories',\n          'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'assessmentyear',\n          'landtaxvaluedollarcnt', 'taxamount', 'taxdelinquencyyear']\n\nobjects = ['airconditioningtypeid', 'architecturalstyletypeid', 'buildingclasstypeid',\n           'buildingqualitytypeid', 'decktypeid', 'fips', 'hashottuborspa', 'heatingorsystemtypeid',\n           'pooltypeid10', 'pooltypeid2', 'pooltypeid7', 'propertycountylandusecode',\n           'propertylandusetypeid', 'propertyzoningdesc', 'rawcensustractandblock', 'regionidcity',\n           'regionidcounty', 'regionidneighborhood', 'regionidzip', 'storytypeid',\n           'typeconstructiontypeid', 'fireplaceflag', 'taxdelinquencyflag', 'censustractandblock']\n\nfeature_dtypes = {col: col_type for type_list, col_type in zip([ints, floats, objects],\n                                                               ['int64', 'float64', 'object']) \n                                  for col in type_list}\n### Let's import our data\ndata = pd.read_csv('../input/properties_2016.csv' , dtype = feature_dtypes)\n### and test if everything OK\ndata.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a882d2da36b80f5193985d22ad1174b7b3dda10"},"cell_type":"code","source":"### ... check for NaNs\nnan = data.isnull().sum()\nnan\n\n### Plotting NaN counts\nnan_sorted = nan.sort_values(ascending=False).to_frame().reset_index()\nnan_sorted.columns = ['Column', 'Number of NaNs']\n\nfig, ax = plt.subplots(figsize=(12, 25))\nsns.barplot(x=\"Number of NaNs\", y=\"Column\", data=nan_sorted, color='Sienna', ax=ax);\nax.set(xlabel=\"Number of NaNs\", ylabel=\"\", title=\"Total Nimber of NaNs in each column\");\n\ndata.dtypes\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"85c4b428b40a83dfee3a1eb17306c8146b3e46df"},"cell_type":"code","source":"continuous = ['basementsqft', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', \n              'finishedsquarefeet12', 'finishedsquarefeet13', 'finishedsquarefeet15',\n              'finishedsquarefeet50', 'finishedsquarefeet6', 'garagetotalsqft', 'latitude',\n              'longitude', 'lotsizesquarefeet', 'poolsizesum',  'yardbuildingsqft17',\n              'yardbuildingsqft26', 'yearbuilt', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt',\n              'landtaxvaluedollarcnt', 'taxamount']\n\ndiscrete = ['bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', 'fireplacecnt', 'fullbathcnt',\n            'garagecarcnt', 'poolcnt', 'roomcnt', 'threequarterbathnbr', 'unitcnt',\n            'numberofstories', 'assessmentyear', 'taxdelinquencyyear']\n### Continuous variable plots\nfor col in continuous:\n    values = data[col].dropna()\n    lower = np.percentile(values, 1)\n    upper = np.percentile(values, 99)\n    fig = plt.figure(figsize=(18,9));\n    sns.distplot(values[(values>lower) & (values<upper)], color='Sienna', ax = plt.subplot(121));\n    sns.boxplot(y=values, color='Sienna', ax = plt.subplot(122));\n    plt.suptitle(col, fontsize=16)       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"93237ec435346ef760dabc4232fcd1c7e2fd73b2"},"cell_type":"code","source":"### Discrete variable plots\nNanAsZero = ['fireplacecnt', 'poolcnt', 'threequarterbathnbr']\nfor col in discrete:\n    if col in NanAsZero:\n        data[col].fillna(0, inplace=True)\n    values = data[col].dropna()   \n    fig = plt.figure(figsize=(18,9));\n    sns.countplot(x=values, color='Sienna', ax = plt.subplot(121));\n    sns.boxplot(y=values, color='Sienna', ax = plt.subplot(122));\n    plt.suptitle(col, fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"192891d82db8f786d4a5f837dfdfadde3d124576"},"cell_type":"code","source":"### Reading train file\nerrors = pd.read_csv('../input/train_2016_v2.csv', parse_dates=['transactiondate'])\nerrors.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9101df5f25cdcac05588942b005c70c443a3fffc"},"cell_type":"code","source":"#### Merging tables\ndata_sold = data.merge(errors, how='inner', on='parcelid')\ndata_sold.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4d40ee8e94649876821eee3066e1e5390709032a"},"cell_type":"code","source":"### Checking logerror\ncol = 'logerror'\n\nvalues = data_sold[col].dropna()\nlower = np.percentile(values, 1)\nupper = np.percentile(values, 99)\nfig = plt.figure(figsize=(18,9));\nsns.distplot(values[(values>lower) & (values<upper)], color='Sienna', ax = plt.subplot(121));\nsns.boxplot(y=values, color='Sienna', ax = plt.subplot(122));\nplt.suptitle(col, fontsize=16);\n\n### Adding some new features from transactiondate\ndata_sold['month'] = data_sold['transactiondate'].dt.month\ndata_sold['day_of_week'] = data_sold['transactiondate'].dt.weekday_name\ndata_sold['week_number'] = data_sold['transactiondate'].dt.week\ndata_sold.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b24a94a63e95178297dd68475372758d13055081"},"cell_type":"code","source":"### Scrutinizing transactiondate\nfig = plt.figure(figsize=(18, 18));\nsns.countplot(x='transactiondate', color='Sienna', data=data_sold, ax = plt.subplot(221));\nsns.countplot(x='month', color='Sienna', data=data_sold, ax = plt.subplot(222));\nsns.countplot(x='day_of_week', color='Sienna', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday',\n                                                      'Friday', 'Saturday', 'Sunday'], \n              data=data_sold, ax = plt.subplot(223));\nsns.countplot(x='week_number', color='Sienna', data=data_sold, ax = plt.subplot(224));\nplt.suptitle('Transaction Date', fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f3c5cc4187211e8d777f9567aed8dcbba803f11d"},"cell_type":"code","source":"### Creating 5 equal size logerror bins \ndata_sold['logerror_bin'] = pd.qcut(data_sold['logerror'], 5, \n                                    labels=['Large Negative Error', 'Medium Negative Error',\n                                            'Small Error', 'Medium Positive Error',\n                                            'Large Positive Error'])\nprint(data_sold.logerror_bin.value_counts())\n\n### Continuous variable vs logerror plots\nfor col in continuous:     \n    fig = plt.figure(figsize=(18,9));\n    sns.barplot(x='logerror_bin', y=col, data=data_sold, ax = plt.subplot(121),\n                order=['Large Negative Error', 'Medium Negative Error','Small Error',\n                       'Medium Positive Error', 'Large Positive Error']);\n    plt.xlabel('LogError Bin');\n    plt.ylabel('Average {}'.format(col));\n    sns.regplot(x='logerror', y=col, data=data_sold, color='Sienna', ax = plt.subplot(122));\n    plt.suptitle('LogError vs {}'.format(col), fontsize=16)   ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}